# Inleiding
*Huisje, boompje, feestje* is een tool die het kopers mogelijk maakt te zien waar de duurste en goedkoopste gebieden zich bevinden en hoe die zich verhouden tot de Nederlandse spoorwegen en -stations. Verkopers kunnen bepalen hoeveel ze voor hun huis kunnen vragen door te zien hoe lang huizen met een specifieke woonoppervlakte in de buurt te koop hebben gestaan en voor welke prijs. 

# Belangrijkste bron van informatie: Funda
Om kopers inzicht te geven in de vraagprijzen van te koop staande huizen en verkopers inzicht te geven in huizen in de buurt die te koop hebben gestaan, is de keuze van informatiebron al snel op [de Funda huizensite](http://www.funda.nl) gevallen. Het webbeheer van Funda zelf gaf aan niet te beschikken over de rechten van de huizeninformatie op de site, maar van de organisatie die dat wel heeft, de Nederlands Vereniging van Makelaars (NVM), heb ik tot op heden nog geen reactie ontvangen op mijn vraag naar toestemming van het gebruik van hun data. Ik weet dat er voorheen een API beschikbaar was om toegang te krijgen tot informatie over huizen op Funda, maar die is nergens meer te vinden. Funda is op het moment bezig met het vernieuwen van de website en dus kan ik me voorstellen dat ze de API hebben verwijderd met het idee dat deze wellicht niet meer goed zou werken. Ik heb besloten de site te scrapen om toegang te krijgen tot de data, vooralsnog dus zonder toestemming van de NVM, maar ik hoop dat nog te krijgen voordat ik de tool daadwerkelijk lanceer.

# Server-side of client-side
Gebruikers moeten met de tool enerzijds een kaart van Nederland op kunnen vragen met postcodegebieden gekleurd op basis van de gemiddelde vraagprijs van die postcode, en anderzijds een overzicht krijgen van de looptijd en vraagprijzen van verkochte huizen in een gekozen postcode met een gekozen woonoppervlakte. In eerste instantie wilde ik de tool zo inrichten dat de data pas gescraped wordt wanneer de gebruiker de site bezoekt, maar het gaat hier om veel data: er staan alleen al 150.000 woonhuizen te koop in Nederland, en dan hebben we het nog niet eens over de andere woontypes en de verkochte huizen in Nederland. Ik wil niet dat gebruikers van de tool lang moeten wachten op de visualisaties, en ik wil Funda ook niet overspoelen met requests naar data. Ik heb er daarom voor gekozen de data eens in de zoveel tijd zelf te scrapen en weg te schrijven naar bestanden, zodat deze kunnen worden gebruikt wanneer een gebruiker de tool bezoekt. 

# Node.js en X-ray
Omdat ik zelf lokaal de data scrape, kan ik gemakkelijk gebruik maken van node.js en de bijbehorende packages. Node module [x-ray](https://github.com/lapwinglabs/x-ray) lijkt me erg geschikt voor het scrapen van de pagina's. Scripts schrijven in deze module werkt heel intuïtief: je geeft een url op, de HTML-elementen waar de informatie vandaan moet komen, de locatie van de *verder*-knop zodat x-ray kan pagineren, een eventueel limiet en het bestand waar x-ray naar moet schrijven. Ook zou x-ray de links op een pagina kunnen opslaan om vervolgens deze pagina's te crawlen. Ik dacht ook de resultaten te kunnen cleanen met x-ray met .prepare() en .format(), maar dat blijkt in de huidige versie nog niet te kunnen. Dat doe ik nu handmatig met regex. 

# Filteren van kopers
De meeste kopers zijn op zoek naar een specifiek type bezit: bijvoorbeeld een woonhuis of een appartement. Het is voor hen daarom het meest interessant een kaart met gemiddelde vraagprijzen op te kunnen vragen van dat type bezit. Tevens beperkt het het aantal resultaten aanzienlijk. Ik kies er daarom voor kopers eerst te laten kiezen voor een type bezit, en dan specifiek een van de vier grootste typen op Funda: woonhuis, appartement, parkeergelegenheid of bouwgrond. Gebruikers kiezen voor een van de vier door middel van een piechart gemaakt met D3Pie. De gebruiker ziet in deze piechart direct hoe de aantallen van aanbod van die typen zich tot elkaar verhouden. Deze data is gescraped met de functie *soort_aanbod_koop* in funda/js/scraper.js. Een tooltip weergeeft het aantal en het percentage van dat type en wanneer de gebruiker op een type klikt, verdwijnt de piechart en verschijnt de kaart. 

# Postcode URL's
Aangezien ik voor de kaart per type de gemiddelde vraagprijs per postcode wilde weten, dacht ik eraan de huizen per postcode te gaan scrapen. Je kunt namelijk bij Funda een postcode invoeren, waarna je alle aangeboden huizen in die postcode te zien krijgt. De benodigde URL verkrijgen is echter niet gemakkelijk. De URL van aangeboden woonhuizen in postcode 1624 is bijvoorbeeld niet http://www.funda.nl/koop/1624/woonhuis/, maar http://www.funda.nl/koop/hoorn-nh/1624/woonhuis/. Er wordt dus een stad- of dorpcode aan toegevoegd, in dit geval *hoorn-nh*. Ik heb geen lijst met deze codes kunnen vinden en dus heb ik het over een andere boeg gegooid. Ik heb een POST request gedaan naar Funda en daarmee eigenlijk steeds gekeken welke Funda teruggeeft wanneer je een postcode invult, en dit naar js/postcodes_met_steden.json geschreven. Omdat ik zoveel request deed, heeft Funda me tijdelijk geblokkeerd en kon ik even niet verder. Nadat ik een delay in het script had verwerkt ging het wel goed, en kreeg ik keurig de benodigde URL's terug. Achteraf heb ik besloten deze URL's niet te gebruiken, en heel Nederland te scrapen naar bijvoorbeeld aangeboden woonhuizen. Anders zou ik namelijk elke postcode apart moeten gaan scrapen en naar een bestand moeten schrijven, wat me niet efficiënt leek. 

# Vraagprijzen scrapen en geheugenproblemen
Nu gebruik ik de data van heel Nederland en bereken ik op basis daarvan per postcode de gemiddelde vraagprijs. Ook dat was niet gemakkelijk. Belangrijk hierbij was eerst de vraagprijzen van een type bezit te scrapen. Dat heb ik gedaan met de functie vraagprijzen() in scraper.js.  Parkeergelegenheid en bouwgrond is goed te doen, maar bij appartementen en woonhuizen krijg ik na een tijd een foutmelding omdat ik geen geheugen meer overhoudt. X-ray klikt als het ware de pagina's door en slaat alle informatie op, maar schrijft het pas aan het einde weg. Ik zou dit op kunnen lossen door een aantal pagina's te scrapen en dan alvast weg te schrijven, maar ik heb ervoor gekozen dit voor nu niet te doen. Ik krijg namelijk evengoed nog heel veel informatie en de bestanden worden al vrij groot. Ik wil niet dat het te lang gaat duren om de kaart op te bouwen en de kaart lijkt nu al goed in de buurt te komen van de werkelijke prijsverschillen in Nederland. Met alle informatie zou het beeld iets completer zijn, maar dit zou ook ten koste gaan van de snelheid. 

# Berekenen gemiddelde vraagprijzen per postcode
Met het bestand met alle vraagprijzen van het aanbod per type bezit en de bijbehorende adressen, kunnen de gemiddelde vraagprijzen per postcode worden berekend. Dit doe ik in *bereken_gemiddelde_vraagprijzen.js*. Belangrijk hierbij was erachter te komen welke postcodes er bestaan in Nederland en met behulp van die postcodes, steeds de vraagprijzen bij elkaar op te tellen en te delen door het totale aantal. Dit heb ik gedaan door postcode/vraagprijs-paren te maken en die te pushen naar een array, en vervolgens de gemiddelde vraagprijzen te berekenen. Waar ik echter steeds tegenaan liep was dat door de synchrone eigenschap van Javascript de array nog niet gevuld was bij de berekening. De array diende eerst te worden gevuld en daarna pas kon het gemiddelde worden berekend. Uiteindelijk is dat gelukt door middel van een promise. 

# Postcodegrenzen met de Geocoders API
Nu de gemiddelde vraagprijzen per postcodes bekend zijn, kan de kaart worden opgemaakt. Daarbij zijn wel de grenzen van de gebieden nodig. Grenzen van Nederlandse gemeentes zijn goed te vinden in shapefiles en json, maar postcodegrenzen bleken moeilijk te vinden. Vaak zijn er kosten verbonden aan dit soort informatie. Na lang zoeken ben ik bij de [GeoCoders API](http://places.geocoders.nl/) van Webmapper terechtgekomen. Ik heb contact opgenomen voor een gebruikersnaam en mocht gelukkig een testaccount hebben. Via deze API kon ik van alle postcodes een geojson opslaan. Deze geojson-bestanden heb ik een voor een weggeschreven naar een bestand in funda/js/geojson/. Als er namelijk een postcodegebied verandert, hoef ik alleen maar de nieuwe geojson naar deze map te schrijven. Wel is het zo dat de geojson-bestanden erg groot bleken te zijn. Met behulp van de ogr2ogr node module heb ik de bestanden aanzienlijk kunnen verkleinen. Handmatig kostte dit teveel tijd om voor alle 899 postcodes te moeten doen, maar ik heb het kunnen automatiseren met een scriptje opgeslagen als *ogr.bat*. 

# Interactieve kaart met Leaflet.js
Met behulp van de gemiddelde vraagprijzen per postcode en de postcode geojsons kon ik de kaart gaan opmaken. Dit wilde ik in eerste instantie met D3.js gaan doen, maar toen ik leaflet.js tegenkwam ben ik daarvoor gegaan. Leaflet.js is als het goed is heel lichtgewicht en biedt veel mogelijkheden wat betreft geojson en verschillende lagen. Met Leaflet kan ik bijvoorbeeld de postcodes met enige transparantie over een basislaag van Mapbox leggen, wat zeker meerwaarde heeft. Hiermee kun je namelijk de wegen, wateren en belangrijke punten zien liggen. De postcodelaag plot ik door elke keer door een geojson heen te gaan, en deze te kleuren op basis van de gemiddelde vraagprijs. De postcodegebieden hadden direct de juiste vorm, maar lagen niet op de juiste plek op de kaart. Uiteindelijk bleek dat door Bootstrap te komen. Door over een postcodegebied heen te gaan kan de gebruiker de betreffende postcode en gemiddelde vraagprijs aflezen. Het had nog wel wat voeten in de aarde om de postcodegebieden de juiste kleur te kunnen geven. In de voorbeelden van Leaflet stond de eigenschap aan de hand waarvan de gebieden gekleurd moesten worden steeds in hetzelfde bestand als de geoinformatie. Daardoor kon gemakkelijk het geojson-bestand mee worden gestuurd aan de style-functie om de kleur te bepalen. Bij mij is dat niet het geval. De informatie moest uit een ander bestand komen, dus die moet worden geopend en steeds worden doorzocht naar de goede gemiddelde vraagprijs. Daarmee kan dan uiteindelijk de juiste kleur worden bepaald met getColor().

# Parkeergelegenheid en bouwgrond laten vallen
De kaart is nu gekleurd met de data van appartementen, maar waar ik niet aan had gedacht, was dat de kleurrange bij parkeerplaatsen bijvoorbeeld heel anders moet. Parkeerplaatsen zijn normaal gesproken veel goedkoper en komen bijna allemaal in de eerste kleurgroep terecht, van 0 tot 50.000 euro. Daarnaast zijn er maar weinig van, wat een vertekenend beeld kan opleveren. Dat geldt eigenlijk ook voor bouwgrond. Er is te weinig aanbod om een realistisch beeld te geven van de gemiddelde prijzen. Dit heeft me doen besluiten me in deze tool uitsluitend te richten op appartementen en woonhuizen. Daar is het meeste aanbod van en is waarschijnlijk ook de meeste interesse voor. De tijd die het me kost om functies te herschrijven zodat parkeerplaatsen en bouwgrond ook goed uitkomen kan ik beter steken in het bieden van meer mogelijkheden voor geïnteresseerden in appartementen en woonhuizen. 

# Verkoopinformatie scrapen en problemen met crawlen
Voor de informatie voor verkopers gebruik ik [d3.layout.timeline](https://github.com/emeeks/d3.layout.timeline). Hiermee kan ik de looptijd van verkochte huizen visualiseren aan de hand van *bands* op een tijdlijn. Daarvoor heb ik wel eerst de verkoopinformatie nodig. Als je op Funda op een verkocht huis klikt, kun je aflezen wanneer het huis is aangeboden, hoe lang het in totaal te koop heeft gestaan en wanneer het is afgemeld/verkocht. Deze informatie heb ik nodig voor de visualisatie. Zoals eerder gezegd zou het met x-ray mogelijk moeten zijn om verder te crawlen vanuit gescrapete links. Na het op verschillende manier geprobeerd te hebben en steeds geen data terug te hebben gekregen, heb ik een issue geplaatst op de projectsite van x-ray. Er werd niet op gereageerd, maar er is vervolgens wel eenzelfde vraag geplaatst. Het blijkt op dit moment niet mogelijk te zijn te crawlen met x-ray. Wel is het zo dat ik zelf de benodigde links bij kan houden en deze steeds kan gebruiken om een nieuwe x-ray scrape te starten. Wat ik nu dus doe is alle verkochte huizen van een type (appartement of woonhuis) scrapen waarna ik de links naar de individuele huizenpagina weer gebruik voor een andere functie, getDetails(). Daar worden de verkoopdetails gescraped, welke weer worden teruggestuurd naar de eerste scrape. Als alle huizen zijn geweest, schrijft de scraper het bestand. 

# Visualiseren met D3 Timeline
D3 timeline layout vraagt om een begindatum en een einddatum, welke er iets anders uit moeten zien dan de data die ik van Funda heb gescraped. Dit kan ik dus beter gelijk formatten in getDetails(). Daarnaast verwacht D3 timeline layout een object met op het eerste niveau een start- en een end-property. Bij mij staan start en end pas op het tweede niveau, namelijk bij object.details.start. Het json-bestand moet dus na de scraper direct worden herschreven om hem geschikt te maken voor de visualisatie. Dit doe ik met de functie herschrijfJson(). Dit alles maakt wel dat de scraper er erg lang over doet. Ik heb ervoor gekozen om niet in een keer alle appartementen of woonhuizen te scrapen, maar ze te verdelen in groepen met dezelfde woonoppervlakte. Zo kan een gebruiker een postcode en een woonoppervlakte kiezen en krijgt het dan een overzicht van verkochte huizen met die woonoppervlakte in die postcode. Anders krijgen de gebruikers namelijk een hele lange lijst met huizen te zien, welke misschien wel veel groter of kleiner zijn dan die van hen en dus niet zo relevant, en bovendien wordt het zo behapbaarder om de data te scrapen. Het scrapen van bijvoorbeeld een appartement met een woonoppervlakte van 80 tot 90m2 kan evengoed nog erg lang duren, soms wel uren. De scraper laat zien dat de verkoopinformatie van de eerste huizen allemaal keurig worden opgeslagen, maar op een gegeven momen komt het tot stilstand  en volgen er een aantal lege objecten. Als ik zelf naar de te scrapen link ga ziet de pagina er heel normaal uit, maar ik denk dat de scraper na een tijd geblokkeerd wordt door Funda en de verkoopinformatie niet meer op kan vragen. Dit kan opgelost worden door de links waar nog geen informatie van bestaat nogmaals af te gaan en de informatie op te vragen. 

Ondanks dat ik lege objecten terug krijg van verkoopdata, geeft D3 timeline layout een mooi beeld van de looptijd van de huizen die ik wel heb kunnen scrapen. De kleur van de bands is gebaseerd op de vraagprijs, en de range van de kleuren gaat van het goedkoopste huis in de tabel naar het duurste huis. Als de range vanaf 0 zou zijn zou er namelijk niet veel verschil in kleuren zijn als de huizen bijvoorbeeld allemaal zo tussen de 150.000 en 175.000 euro kosten. 

# Dynamisch schalen
Wanneer de tabel maar enkele verkochte huizen bevatte zag het er goed uit, maar later viel het me wel op dat het mis ging als er meer of minder huizen in de tabel stonden. De SVG had namelijk een vast breedte en hoogte, dus als er meer bands waren werden die eraf geknipt. Ik heb dat opgelost door de hoogte van de SVG dynamisch aan te passen aan de hand van het aantal bands. Zo komen alle bands er netjes op te staan en is er nooit veel witruimte onderaan. Ook als er huizen waren met een hele lange looptijd ontstond er een probleem. Ik had gekozen voor een tick per maand, maar als de looptijd vele maanden bestreek, gingen de ticks elkaar overlappen. Gelukkig kan ik binnen D3 timeline rekenen met datums, dus ik kan de eerste en laatste datum opvragen, waarna het verschil kan worden uitgedrukt in een aantal miljarden. Nu is het lastig te werken met zulke grote getallen, maar als je het deelt door een miljard krijg je werkbare getallen en kan worden bepaald hoeveel ticks er moeten worden aangemaakt. Als er dus huizen tussen zitten met een hele lange looptijd, worden er ticks aangemaakt van een halfjaar. Bij middelmatige looptijd komt er een tijd per kwartaal, en bij korte looptijd een per maand. Hierdoor is er genoeg ruimte tussen de ticks. 

# Extra laag spoorwegen
Tot slot was mijn wens nog om kopers de mogelijkheid te bieden lagen aan en uit te zetten om te zien hoe postcodes zich verhouden tot spoorwegen en snelwegen. Het is erg moeilijk gebleken goede geodata te vinden hierover. Uiteindelijk heb ik wel een goed bestand met spoorwegen kunnen vinden. Deze is toegevoegd als laag, waardoor deze kan worden aangevink wanneer gewenst. 

# Toekomst
Binnenkort wil ik de tool lanceren en zoveel mogelijk mensen vragen hem te gebruiken. Zo zullen er nog wel een aantal bugs naar voren komen die ik nog niet tegen ben gekomen. Ik wil zowel het kopers gedeelte als het verkopers gedeelte ook geschikt maken voor informatie over huurhuizen, want ook dat lijkt me heel waardevol. Daarnaast wil ik de ontbrekende data nog scrapen en hopelijk krijg ik nog officiële toestemming van de NVM. Om met een gerust hart de tool te lanceren vind ik dat toch een belangrijk punt. Wellicht vinden ze het wel interessant en doen ze hiermee ideeën op voor Funda. Ik ben zelf in ieder geval tot nieuwe inzichten gekomen en heb nu een goed idee van waar ik later wil gaan wonen, maar eerst nog even sparen!